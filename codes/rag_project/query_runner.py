import json
import requests
from langchain_community.vectorstores import Chroma

# Ollama ì„œë²„ ì •ë³´
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_MODEL = "deepseek-r1:8b"

def query_ollama(prompt):
    """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ deepseek-r1:8b ëª¨ë¸ í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬)"""
    try:
        # í”„ë¡¬í”„íŠ¸ì— í•œê¸€ ì‘ë‹µ ìš”ì²­ ì¶”ê°€
        korean_prompt = f"""
ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì„ ì—„ê²©íˆ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ì˜ì–´ ë‹¨ì–´ëŠ” ëª¨ë‘ í•œê¸€ë¡œ ë³€í™˜í•˜ì„¸ìš” (ì˜ˆ: AI -> ì—ì´ì•„ì´).
3. íŠ¹ìˆ˜ë¬¸ìë‚˜ í•œìëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
4. ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
5. ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë¶€ì—°ì€ ì œì™¸í•˜ì„¸ìš”.

ì§ˆë¬¸:
{prompt}

ë‹µë³€ í˜•ì‹:
[ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ í•œê¸€ ë‹µë³€ë§Œ ì‘ì„±]
"""
        response = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": korean_prompt,
                "system": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ìš© ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì ˆëŒ€ì ìœ¼ë¡œ ë”°ë¥´ì„¸ìš”:\n1. ì˜¤ì§ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”\n2. ì˜ì–´ëŠ” í•œê¸€ë¡œ ë³€í™˜í•˜ì„¸ìš” (AI -> ì—ì´ì•„ì´)\n3. íŠ¹ìˆ˜ë¬¸ìì™€ í•œìëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”\n4. ê°„ë‹¨ëª…ë£Œí•˜ê²Œ í•µì‹¬ë§Œ ë‹µë³€í•˜ì„¸ìš”\n5. ëª¨ë“  ì™¸ë˜ì–´ëŠ” í•œê¸€ë¡œ í‘œê¸°í•˜ì„¸ìš”\n6. ë‹µë³€ ì´ì™¸ì˜ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”"
            },
            stream=True  # Enable streaming for better response handling
        )
        
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_response = json.loads(line)
                if 'response' in json_response:
                    full_response += json_response['response']
        
        # ì¶”ë¡  ê³¼ì •ì„ ì œê±°í•˜ê³  ìµœì¢… í•œê¸€ ë‹µë³€ë§Œ ì¶”ì¶œ
        if '\n\n' in full_response:
            final_response = full_response.split('\n\n')[-1]
        else:
            final_response = full_response
            
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        final_response = final_response.strip().replace('"', '')
        return final_response if final_response else "âš ï¸ Ollama ì‘ë‹µ ì˜¤ë¥˜"

    except requests.exceptions.RequestException as e:
        return f"ğŸš¨ Ollama ì—°ê²° ì˜¤ë¥˜: {e}"

def run_rag_query(vector_db, query):
    """ë²¡í„°DBì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê²€ìƒ‰"""
    try:
        # ë²¡í„°DBì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        results = vector_db.similarity_search(query, k=3)
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        context = "\n\n".join([doc.page_content for doc in results])
        
        # ê°„ë‹¨í•œ ìƒíƒœ ë©”ì‹œì§€ ì¶œë ¥
        print(f"\nğŸ’¾ ë²¡í„° DB ì½œë ‰ì…˜ ì¡°íšŒ ì„±ê³µ")
        print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ {len(results)}ê°œ ì°¾ìŒ")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ë¬¸ì„œ ë‚´ìš©ì€ ë‚´ë¶€ì ìœ¼ë¡œë§Œ ì‚¬ìš©
        prompt = f"""ì—­í• : ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ì¤‘ìš” ì§€ì¹¨:
1. ë°˜ë“œì‹œ ìœ„ì˜ ë¬¸ì„œ ë‚´ìš©ì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
3. ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì¶”ë¡ ì„ í†µí•œ ë‹µë³€ì€ í•˜ì§€ ë§ˆì„¸ìš”.
4. ì˜ì–´ë‚˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
5. ë¬¸ì„œì—ì„œ ì°¾ì€ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
6. ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€: """
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Ollama APIë¡œ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
        response = query_ollama(prompt)
        print(f"\nğŸ’¬ ë‹µë³€: {response}\n")
        return response
    except Exception as e:
        print(f"âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "âš ï¸ RAG ê²€ìƒ‰ ì˜¤ë¥˜"
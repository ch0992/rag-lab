{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG ì‹œìŠ¤í…œ - ë¬¸ì„œ ì§ˆì˜ ë° ì‘ë‹µ (Document Query & Response)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì €ì¥ëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  LLMì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” êµ¬ì„±ìš”ì†Œ\n",
    "1. ë²¡í„° ê²€ìƒ‰ (Vector Search)\n",
    "2. LLM ì—°ë™ (Language Model Integration)\n",
    "3. ì‘ë‹µ ìƒì„± (Response Generation)\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "- ChromaDB ì„œë²„ ì‹¤í–‰ ì¤‘\n",
    "- Ollama ì„œë²„ ì‹¤í–‰ ì¤‘\n",
    "- deepseek-r1:8b ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •\n",
    "\n",
    "RAG ì‹œìŠ¤í…œì˜ ì§ˆì˜-ì‘ë‹µì— í•„ìš”í•œ ì„¤ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤:\n",
    "- requests: Ollama API í˜¸ì¶œ\n",
    "- Chroma: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤\n",
    "- ì„œë²„ ì„¤ì • ë° ëª¨ë¸ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import requests\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Ollama ì„œë²„ ì„¤ì •\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"deepseek-r1:8b\"\n",
    "\n",
    "# ChromaDB ì„¤ì •\n",
    "PERSIST_DIR = \"./data\"\n",
    "COLLECTION_NAME = \"rag_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤:\n",
    "- nomic-embed-text-v1 ëª¨ë¸\n",
    "- CPU ê¸°ë°˜ ì²˜ë¦¬\n",
    "- ì •ê·œí™”ëœ ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_embeddings():\n",
    "    \"\"\"í•œêµ­ì–´ì— ìµœì í™”ëœ ì„ë² ë”© ëª¨ë¸ ìƒì„±\"\"\"\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\n",
    "\n",
    "ChromaDBì— ì—°ê²°í•˜ì—¬ ì €ì¥ëœ ë¬¸ì„œì— ì ‘ê·¼í•©ë‹ˆë‹¤:\n",
    "- ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "- ì§€ì •ëœ ì»¬ë ‰ì…˜ ì—°ê²°\n",
    "- ì—ëŸ¬ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_vector_store():\n",
    "    \"\"\"ChromaDBì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\"\"\"\n",
    "    try:\n",
    "        embedding_model = get_embeddings()\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=PERSIST_DIR,\n",
    "            embedding_function=embedding_model,\n",
    "            collection_name=COLLECTION_NAME\n",
    "        )\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ollama API í˜¸ì¶œ\n",
    "\n",
    "LLM ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤:\n",
    "- í•œê¸€ ì‘ë‹µ ìµœì í™”\n",
    "- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬\n",
    "- ì‘ë‹µ ì •ì œ ë° í¬ë§·íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ëª¨ë¸ í˜¸ì¶œ\"\"\"\n",
    "    try:\n",
    "        # í”„ë¡¬í”„íŠ¸ì— í•œê¸€ ì‘ë‹µ ìš”ì²­ ì¶”ê°€\n",
    "        korean_prompt = f\"\"\"\n",
    "ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì„ ì—„ê²©íˆ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "1. ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "2. ì˜ì–´ ë‹¨ì–´ëŠ” ëª¨ë‘ í•œê¸€ë¡œ ë³€í™˜í•˜ì„¸ìš” (ì˜ˆ: API -> ì—ì´í”¼ì•„ì´).\n",
    "3. íŠ¹ìˆ˜ë¬¸ìë‚˜ í•œìëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "4. ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "5. ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë¶€ì—°ì€ ì œì™¸í•˜ì„¸ìš”.\n",
    "6. ë‹µë³€ ì „ì— ìƒê°í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì£¼ì§€ ë§ˆì„¸ìš”.\n",
    "7. ë°”ë¡œ ê²°ê³¼ë§Œ ë³´ì—¬ì£¼ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{prompt}\n",
    "\n",
    "ë‹µë³€ í˜•ì‹:\n",
    "[ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë§Œ ì‘ì„±]\n",
    "\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/generate\",\n",
    "            json={\n",
    "                \"model\": OLLAMA_MODEL,\n",
    "                \"prompt\": korean_prompt,\n",
    "                \"system\": \"ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ìš© ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\"\n",
    "            },\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                json_response = json.loads(line)\n",
    "                if 'response' in json_response:\n",
    "                    full_response += json_response['response']\n",
    "        \n",
    "        # ì‘ë‹µ ì •ë¦¬\n",
    "        full_response = full_response.replace('<think>\\n', '').replace('</think>', '')\n",
    "        if '[' in full_response:\n",
    "            full_response = full_response.split(']')[-1].strip()\n",
    "            \n",
    "        return full_response.strip()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"ğŸš¨ Ollama ì—°ê²° ì˜¤ë¥˜: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG ì§ˆì˜ ì‹¤í–‰\n",
    "\n",
    "ì „ì²´ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤:\n",
    "1. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°\n",
    "2. ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "3. í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "4. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_rag_query(query):\n",
    "    \"\"\"ë²¡í„°DBì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê²€ìƒ‰\"\"\"\n",
    "    try:\n",
    "        # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\n",
    "        vector_store = load_vector_store()\n",
    "        if not vector_store:\n",
    "            return \"ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "        # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        results = vector_store.similarity_search(query, k=3)\n",
    "        \n",
    "        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì²˜ë¦¬\n",
    "        contexts = []\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            contexts.append(f\"ë¬¸ì„œ {i}:\\n{doc.page_content}\")\n",
    "        \n",
    "        context = \"\\n\\n=== ë‹¤ìŒ ë¬¸ì„œ ===\\n\\n\".join(contexts)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ ë²¡í„° DB ì½œë ‰ì…˜ ì¡°íšŒ ì„±ê³µ\")\n",
    "        print(f\"ğŸ” ê´€ë ¨ ë¬¸ì„œ {len(results)}ê°œ ì°¾ìŒ\")\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        prompt = f\"\"\"ì—­í• : ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì—ì„œ ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¬¸ì„œ ë‚´ìš©:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "\n",
    "ì¤‘ìš” ì§€ì¹¨:\n",
    "1. ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê²€í† í•˜ì—¬ ê´€ë ¨ëœ ì •ë³´ë¥¼ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.\n",
    "2. ê° ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n",
    "3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "4. ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì¶”ë¡ ì€ í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "5. ì˜ì–´ë‚˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ìµœì†Œí•œìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "6. ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "7. ë‹µë³€ì€ ê°„ê²°í•˜ë©´ì„œë„ í¬ê´„ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "8. ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ë‹µë³€: \"\"\"\n",
    "        \n",
    "        # Ollamaë¡œ ë‹µë³€ ìƒì„±\n",
    "        response = query_ollama(prompt)\n",
    "        print(f\"\\nğŸ’¬ ë‹µë³€: {response}\\n\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return \"âš ï¸ RAG ê²€ìƒ‰ ì˜¤ë¥˜\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì˜ˆì œ: ì§ˆë¬¸í•˜ê¸°\n",
    "\n",
    "ì‹¤ì œ ì§ˆë¬¸ì„ í†µí•´ RAG ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\n",
    "- ê°„ë‹¨í•œ ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "- ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶œë ¥\n",
    "- ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ì˜ˆì œ: ì§ˆë¬¸í•˜ê¸°\n",
    "question = \"í•œêµ­ì˜ ë´„ì€ ì–´ë–¤ íŠ¹ì§•ì´ ìˆë‚˜ìš”?\"\n",
    "answer = run_rag_query(question)\n",
    "print(f\"ì§ˆë¬¸: {question}\")\n",
    "print(f\"ë‹µë³€: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 시스템 - 문서 질의 및 응답 (Document Query & Response)\n",
    "\n",
    "이 노트북은 저장된 문서에 대해 질문하고 LLM을 통해 답변을 생성하는 과정을 설명합니다.\n",
    "\n",
    "## 주요 구성요소\n",
    "1. 벡터 검색 (Vector Search)\n",
    "2. LLM 연동 (Language Model Integration)\n",
    "3. 응답 생성 (Response Generation)\n",
    "\n",
    "## 사전 요구사항\n",
    "- ChromaDB 서버 실행 중\n",
    "- Ollama 서버 실행 중\n",
    "- deepseek-r1:8b 모델 다운로드 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 임포트 및 설정\n",
    "\n",
    "RAG 시스템의 질의-응답에 필요한 설정을 진행합니다:\n",
    "- requests: Ollama API 호출\n",
    "- Chroma: 벡터 데이터베이스\n",
    "- 서버 설정 및 모델 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import requests\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Ollama 서버 설정\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"deepseek-r1:8b\"\n",
    "\n",
    "# ChromaDB 설정\n",
    "PERSIST_DIR = \"./data\"\n",
    "COLLECTION_NAME = \"rag_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 임베딩 모델 설정\n",
    "\n",
    "문서 검색을 위한 임베딩 모델을 준비합니다:\n",
    "- nomic-embed-text-v1 모델\n",
    "- CPU 기반 처리\n",
    "- 정규화된 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_embeddings():\n",
    "    \"\"\"한국어에 최적화된 임베딩 모델 생성\"\"\"\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터 스토어 로드\n",
    "\n",
    "ChromaDB에 연결하여 저장된 문서에 접근합니다:\n",
    "- 임베딩 모델 초기화\n",
    "- 지정된 컬렉션 연결\n",
    "- 에러 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_vector_store():\n",
    "    \"\"\"ChromaDB에서 벡터 스토어 로드\"\"\"\n",
    "    try:\n",
    "        embedding_model = get_embeddings()\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=PERSIST_DIR,\n",
    "            embedding_function=embedding_model,\n",
    "            collection_name=COLLECTION_NAME\n",
    "        )\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 벡터 스토어 로드 실패: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ollama API 호출\n",
    "\n",
    "LLM 모델을 호출하여 응답을 생성합니다:\n",
    "- 한글 응답 최적화\n",
    "- 스트리밍 응답 처리\n",
    "- 응답 정제 및 포맷팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Ollama API를 사용하여 LLM 모델 호출\"\"\"\n",
    "    try:\n",
    "        # 프롬프트에 한글 응답 요청 추가\n",
    "        korean_prompt = f\"\"\"\n",
    "다음 지시사항을 엄격히 따라 답변해주세요:\n",
    "\n",
    "1. 반드시 한글로만 답변하세요.\n",
    "2. 영어 단어는 모두 한글로 변환하세요 (예: API -> 에이피아이).\n",
    "3. 특수문자나 한자는 절대 사용하지 마세요.\n",
    "4. 간단명료하게 답변하세요.\n",
    "5. 불필요한 설명이나 부연은 제외하세요.\n",
    "6. 답변 전에 생각하는 과정을 보여주지 마세요.\n",
    "7. 바로 결과만 보여주세요.\n",
    "\n",
    "질문:\n",
    "{prompt}\n",
    "\n",
    "답변 형식:\n",
    "[질문에 대한 답변만 작성]\n",
    "\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/generate\",\n",
    "            json={\n",
    "                \"model\": OLLAMA_MODEL,\n",
    "                \"prompt\": korean_prompt,\n",
    "                \"system\": \"당신은 한국어 전용 답변 도우미입니다.\"\n",
    "            },\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                json_response = json.loads(line)\n",
    "                if 'response' in json_response:\n",
    "                    full_response += json_response['response']\n",
    "        \n",
    "        # 응답 정리\n",
    "        full_response = full_response.replace('<think>\\n', '').replace('</think>', '')\n",
    "        if '[' in full_response:\n",
    "            full_response = full_response.split(']')[-1].strip()\n",
    "            \n",
    "        return full_response.strip()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"🚨 Ollama 연결 오류: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG 질의 실행\n",
    "\n",
    "전체 RAG 프로세스를 실행하는 함수입니다:\n",
    "1. 벡터 검색으로 관련 문서 찾기\n",
    "2. 문서 컨텍스트 구성\n",
    "3. 프롬프트 생성\n",
    "4. LLM으로 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_rag_query(query):\n",
    "    \"\"\"벡터DB에서 질문에 대한 답변을 검색\"\"\"\n",
    "    try:\n",
    "        # 벡터 스토어 로드\n",
    "        vector_store = load_vector_store()\n",
    "        if not vector_store:\n",
    "            return \"벡터 스토어를 로드할 수 없습니다.\"\n",
    "\n",
    "        # 유사도 검색 수행\n",
    "        results = vector_store.similarity_search(query, k=3)\n",
    "        \n",
    "        # 검색된 문서 처리\n",
    "        contexts = []\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            contexts.append(f\"문서 {i}:\\n{doc.page_content}\")\n",
    "        \n",
    "        context = \"\\n\\n=== 다음 문서 ===\\n\\n\".join(contexts)\n",
    "        \n",
    "        print(f\"\\n💾 벡터 DB 콜렉션 조회 성공\")\n",
    "        print(f\"🔍 관련 문서 {len(results)}개 찾음\")\n",
    "        \n",
    "        # 프롬프트 구성\n",
    "        prompt = f\"\"\"역할: 당신은 주어진 문서들에서 모든 관련 정보를 찾아 종합적으로 답변하는 역할을 합니다.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "중요 지침:\n",
    "1. 모든 문서의 내용을 검토하여 관련된 정보를 모두 찾아주세요.\n",
    "2. 각 문서의 정보를 종합하여 하나의 완성된 답변을 만들어주세요.\n",
    "3. 문서에 없는 내용은 추가하지 마세요.\n",
    "4. 외부 지식이나 추론은 하지 마세요.\n",
    "5. 영어나 특수문자는 최소한으로 사용하세요.\n",
    "6. 여러 문서의 정보가 있다면 모두 포함해서 답변해주세요.\n",
    "7. 답변은 간결하면서도 포괄적이어야 합니다.\n",
    "8. 문서에서 관련 내용을 찾을 수 없다면 '주어진 문서에서 관련 정보를 찾을 수 없습니다.'라고만 답변해주세요.\n",
    "\n",
    "답변: \"\"\"\n",
    "        \n",
    "        # Ollama로 답변 생성\n",
    "        response = query_ollama(prompt)\n",
    "        print(f\"\\n💬 답변: {response}\\n\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"❌ RAG 검색 중 오류 발생: {e}\")\n",
    "        return \"⚠️ RAG 검색 오류\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 예제: 질문하기\n",
    "\n",
    "실제 질문을 통해 RAG 시스템을 테스트합니다:\n",
    "- 간단한 질문 예시\n",
    "- 질문과 답변 출력\n",
    "- 시스템 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 예제: 질문하기\n",
    "question = \"한국의 봄은 어떤 특징이 있나요?\"\n",
    "answer = run_rag_query(question)\n",
    "print(f\"질문: {question}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“¦ Step 06: Elasticsearchì— ë²¡í„° ë°ì´í„° ì €ì¥\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±í•œ ë²¡í„°í™”ëœ ë¬¸ì„œë“¤ì„ Elasticsearchì— ì €ì¥í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“ ì£¼ìš” ë‚´ìš©\n",
    "1. Elasticsearch ì—°ê²° ì„¤ì •\n",
    "2. ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘ ì„¤ì •\n",
    "3. ë²¡í„°í™”ëœ ë¬¸ì„œ ì €ì¥\n",
    "4. ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ ë° ê´€ë¦¬\n",
    "\n",
    "## ğŸ’¡ Elasticsearch ë²¡í„° ê²€ìƒ‰ ì´í•´í•˜ê¸°\n",
    "\n",
    "Elasticsearchì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ì¸ë±ìŠ¤ ë§¤í•‘**: dense_vector íƒ€ì…ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° í•„ë“œë¥¼ ì •ì˜\n",
    "2. **ì°¨ì› ì„¤ì •**: OpenAIì˜ text-embedding-ada-002 ëª¨ë¸ì€ 1536 ì°¨ì› ë²¡í„° ìƒì„±\n",
    "3. **ìœ ì‚¬ë„ ë©”íŠ¸ë¦­**: cosine similarityë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "4. **ì¸ë±ìŠ¤ ì„¤ì •**: íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ìœ„í•œ ìƒ¤ë“œ ë° ë ˆí”Œë¦¬ì¹´ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •\n",
    "\n",
    "Elasticsearch ì—°ê²°ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³  í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤\n",
    "load_dotenv()\n",
    "\n",
    "# Elasticsearch í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "es = Elasticsearch(\n",
    "    hosts=[os.getenv('ELASTICSEARCH_URL', 'http://localhost:9200')],\n",
    "    basic_auth=(\n",
    "        os.getenv('ELASTICSEARCH_USERNAME', 'elastic'),\n",
    "        os.getenv('ELASTICSEARCH_PASSWORD', '')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Elasticsearch ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤\n",
    "if es.ping():\n",
    "    print(\"âœ… Elasticsearchì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    raise ConnectionError(\"âŒ Elasticsearch ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘ ì„¤ì •\n",
    "\n",
    "Elasticsearchì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "- dense_vector íƒ€ì…ìœ¼ë¡œ ì„ë² ë”© í•„ë“œë¥¼ ì •ì˜\n",
    "- ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ìœ„í•œ í•„ë“œ ì¶”ê°€\n",
    "- íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì •\n",
    "INDEX_NAME = \"metadata-embeddings\"\n",
    "\n",
    "# ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        # ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìƒ¤ë“œ ì„¤ì •\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            # ë¬¸ì„œ ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            # ë¬¸ì„œ ì œëª© ë˜ëŠ” ì‹ë³„ì\n",
    "            \"title\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            # ë¬¸ì„œ ì¢…ë¥˜ (í…Œì´ë¸”, ì»¬ëŸ¼ ë“±)\n",
    "            \"type\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            # OpenAI ì„ë² ë”© ë²¡í„° (1536 ì°¨ì›)\n",
    "            \"embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1536,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ì‚­ì œ\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"ğŸ—‘ï¸ ê¸°ì¡´ '{INDEX_NAME}' ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìƒˆ ì¸ë±ìŠ¤ ìƒì„±\n",
    "es.indices.create(index=INDEX_NAME, body=mapping)\n",
    "print(f\"âœ¨ ìƒˆë¡œìš´ '{INDEX_NAME}' ì¸ë±ìŠ¤ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ë²¡í„°í™”ëœ ë¬¸ì„œ ë¡œë“œ\n",
    "\n",
    "ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±í•œ ë²¡í„°í™”ëœ ë¬¸ì„œë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„°í™”ëœ ë¬¸ì„œ íŒŒì¼ ë¡œë“œ\n",
    "with open('../data/processed/documents_with_embeddings.json', 'r', encoding='utf-8') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
    "print(\"\\në¬¸ì„œ êµ¬ì¡° ì˜ˆì‹œ:\")\n",
    "sample_doc = documents[0]\n",
    "print(f\"- ë‚´ìš©: {sample_doc['content'][:100]}...\")\n",
    "print(f\"- ì„ë² ë”© ì°¨ì›: {len(sample_doc['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Elasticsearchì— ë¬¸ì„œ ì €ì¥\n",
    "\n",
    "ë²¡í„°í™”ëœ ë¬¸ì„œë“¤ì„ Elasticsearchì— íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- bulk APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬\n",
    "- ì§„í–‰ ìƒí™©ì„ tqdmìœ¼ë¡œ ì‹œê°í™”\n",
    "- ì—ëŸ¬ ì²˜ë¦¬ ë° ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bulk_actions(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Elasticsearch bulk APIìš© ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        documents: ë²¡í„°í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        bulk API í¬ë§·ì˜ ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        # ê° ë¬¸ì„œì— ëŒ€í•œ ì¸ë±ìŠ¤ ì•¡ì…˜ ì •ì˜\n",
    "        yield {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_id\": str(i),  # ë¬¸ì„œ ID\n",
    "            \"_source\": {\n",
    "                \"content\": doc[\"content\"],\n",
    "                \"title\": doc.get(\"title\", f\"ë¬¸ì„œ_{i}\"),\n",
    "                \"type\": doc.get(\"type\", \"metadata\"),\n",
    "                \"embedding\": doc[\"embedding\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# bulk APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì €ì¥\n",
    "try:\n",
    "    with tqdm(total=len(documents), desc=\"ë¬¸ì„œ ì €ì¥ ì¤‘\") as pbar:\n",
    "        success, failed = bulk(\n",
    "            es,\n",
    "            generate_bulk_actions(documents),\n",
    "            chunk_size=100,  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜\n",
    "            refresh=True,    # ì €ì¥ í›„ ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •\n",
    "            raise_on_error=True\n",
    "        )\n",
    "        pbar.update(success)\n",
    "    \n",
    "    print(f\"âœ… ì´ {success}ê°œ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë¬¸ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸\n",
    "\n",
    "ì €ì¥ëœ ë°ì´í„°ì˜ ìƒíƒœì™€ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸\n",
    "index_stats = es.indices.stats(index=INDEX_NAME)\n",
    "index_info = es.indices.get(index=INDEX_NAME)\n",
    "\n",
    "print(\"ğŸ“Š ì¸ë±ìŠ¤ ìƒíƒœ:\")\n",
    "print(f\"- ë¬¸ì„œ ìˆ˜: {index_stats['_all']['total']['docs']['count']}\")\n",
    "print(f\"- ì €ì¥ í¬ê¸°: {index_stats['_all']['total']['store']['size_in_bytes'] / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œ ê²€ìƒ‰ìœ¼ë¡œ ì €ì¥ í™•ì¸\n",
    "sample = es.search(\n",
    "    index=INDEX_NAME,\n",
    "    body={\n",
    "        \"size\": 1,\n",
    "        \"query\": {\"match_all\": {}}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“„ ì €ì¥ëœ ë¬¸ì„œ ìƒ˜í”Œ:\")\n",
    "print(json.dumps(sample['hits']['hits'][0]['_source'], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì •ë¦¬\n",
    "\n",
    "ì´ë²ˆ ë‹¨ê³„ì—ì„œ ì™„ë£Œí•œ ì‘ì—…:\n",
    "1. Elasticsearch ì—°ê²° ë° ì„¤ì •\n",
    "2. ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜\n",
    "3. ë²¡í„°í™”ëœ ë¬¸ì„œì˜ íš¨ìœ¨ì ì¸ ì €ì¥\n",
    "4. ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ ë° ê²€ì¦\n",
    "\n",
    "ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì €ì¥ëœ ë²¡í„° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ \n",
    "ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
